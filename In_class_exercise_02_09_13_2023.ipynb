{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreelakshmi0702/SreeLakshmi_INFO5731_Fall2023/blob/main/In_class_exercise_02_09_13_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAzh1U0sE5I5"
      },
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpgvZQdRE-HV"
      },
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "Research question:\n",
        ". Create a topic model using the subjects that the cricket player discussed in the interview.\n",
        ". Create the interview data.\n",
        "\n",
        "We can gather 1042 interview samples from the following link: https://www.cricketinterviews.com/\n",
        "\n",
        "\n",
        "Steps for collecting and saving the data:\n",
        ". We use web scraping technique to scrape the data.\n",
        ". We use BeautifulSoup library to parse the Densho Digital Repository.\n",
        ". We parse through each page and go to the link and then to interviews hyperlink.\n",
        ". We get the info in homepage."
      ],
      "metadata": {
        "id": "nqhsOu-wc7WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QpWOgjHi9-pa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# You code here (Please add comments in the code):\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError\n",
        "import json\n",
        "import re\n",
        "\n",
        "total_count = 0\n",
        "count = 0\n",
        "interview_count = 0\n",
        "# url of the densho digital repository to scrape the data\n",
        "url = \"https://www.cricketinterviews.com/\"\n",
        "for page_num in range(1, 38):\n",
        "    # we create the data soup of the main url page html elements.\n",
        "    link1 = Request(url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    #url1 = urlopen(link1)\n",
        "\n",
        "    data1 = url1.read()\n",
        "    data1_soup = BeautifulSoup(data1)\n",
        "\n",
        "    #print(\"*** Page {} ***\".format(page_num))\n",
        "    # we iterate through each narrator page link\n",
        "    for narrator_link in data1_soup.find_all('h4'):\n",
        "        link2 = Request(narrator_link.a.get('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        url2 = urlopen(link2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5rjlclf9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "621d74a7-3320-4925-bc95-f5f28db186bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Titles  \\\n",
            "0    [PDF][PDF] Evaluation of evaluation in informa...   \n",
            "1          [BOOK][B] Information retrieval interaction   \n",
            "2           [CITATION][C] Modern information retrieval   \n",
            "3    A definition of relevance for information retr...   \n",
            "4           [BOOK][B] Information retrieval evaluation   \n",
            "..                                                 ...   \n",
            "185  [BOOK][B] Information extraction: algorithms a...   \n",
            "186                  Distributed information retrieval   \n",
            "187  [BOOK][B] Information retrieval: data structur...   \n",
            "188  [BOOK][B] Information retrieval for music and ...   \n",
            "189        Online evaluation for information retrieval   \n",
            "\n",
            "                                       Author_and_Year  \\\n",
            "0    T Saracevic - … on Research and development in...   \n",
            "1             P Ingwersen - 1992 - peteringwersen.info   \n",
            "2          BY Ricardo - 1999 - Pearson Education India   \n",
            "3    WS Cooper - Information storage and retrieval,...   \n",
            "4                   D Harman - 2011 - books.google.com   \n",
            "..                                                 ...   \n",
            "185                         MF Moens - 2006 - Springer   \n",
            "186  J Callan - … from the Center for Intelligent I...   \n",
            "187       WB Frakes, R Baeza-Yates - 1992 - dl.acm.org   \n",
            "188                         M Müller - 2007 - Springer   \n",
            "189  K Hofmann, L Li, F Radlinski - … ® in Informat...   \n",
            "\n",
            "                                              Abstract  \n",
            "0    ABSTRACT Evaluation is a major force in resear...  \n",
            "1    … Information retrieval covers the problems re...  \n",
            "2                                                       \n",
            "3    … The purpose of this paper, then, is to propo...  \n",
            "4    … played a major role in information retrieval...  \n",
            "..                                                 ...  \n",
            "185  … of information extraction in retrieval syste...  \n",
            "186  A multi-database model of distributed informat...  \n",
            "187  … covering the major information retrieval alg...  \n",
            "188  … in music information retrieval. In … retriev...  \n",
            "189  … with real world information needs to play an...  \n",
            "\n",
            "[190 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# url of the densho digital repository to scrape the data\n",
        "url = \"https://scholar.google.com/scholar?start={}&q=information+retrieval&hl=en&as_sdt=0,44\"\n",
        "df = pd.DataFrame()\n",
        "dict = {\"Titles\": [], \"Author_and_Year\": [], \"Abstract\": []}\n",
        "for num in range(1, 20):\n",
        "    # we create the data soup of the main url page html elements.\n",
        "    num=num + 9\n",
        "    link1 = Request(url.format(num), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    url1 = urlopen(link1)\n",
        "    data1 = url1.read()\n",
        "    data1_soup = BeautifulSoup(data1)\n",
        "\n",
        "    title_list = data1_soup.find_all(\"h3\", {\"class\": \"gs_rt\"})\n",
        "    author_year_list = data1_soup.find_all(\"div\", {\"class\": \"gs_a\"})\n",
        "    abstract_list = data1_soup.find_all(\"div\", {\"class\": \"gs_rs\"})\n",
        "\n",
        "    for title in title_list:\n",
        "        dict['Titles'].append(title.text)\n",
        "    for author_year in author_year_list:\n",
        "        dict['Author_and_Year'].append(author_year.text)\n",
        "    for abstract in abstract_list:\n",
        "        dict['Abstract'].append(abstract.text)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(dict)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FymVNKVi9-pb"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "import tweepy\n",
        "import csv\n",
        "import pandas as pd\n",
        "####input your credentials here\n",
        "consumer_key = 'u7L11nR7HN85dn1qnTFO1cegb'\n",
        "consumer_secret = 'QN1JrEmit2To46ZcwWAT4aI5QGWZXWRDDUPnMCWV5M66SFc8wT'\n",
        "access_token = '1144377060036620294-BSEicX3zH7hIhksbNZV9mrWFwa07co'\n",
        "access_token_secret = 'gxWMOodDq1nQAjix9mHEOUSAtgE7XH5ctHInm0XRs1Jce'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "#####United Airlines\n",
        "# Open/Create a file to append data\n",
        "csvFile = open('ua.csv', 'a')\n",
        "#Use csv Writer\n",
        "csvWriter = csv.writer(csvFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH8tDEAeWges"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}